{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Mental Health Demo.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z3QZJ0NvEJYy",
        "outputId": "de4db04b-46b7-4f7a-bacc-9fec87acabaf"
      },
      "source": [
        "import pickle\n",
        "import nltk\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "import pandas as pd\n",
        "import re\n",
        "from keras_preprocessing.sequence import pad_sequences\n",
        "import os\n",
        "!pip install gradio\n",
        "import gradio as gr"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting gradio\n",
            "  Downloading gradio-3.9.1-py3-none-any.whl (11.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 11.6 MB 4.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from gradio) (2.23.0)\n",
            "Collecting python-multipart\n",
            "  Downloading python-multipart-0.0.5.tar.gz (32 kB)\n",
            "Collecting pydub\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Collecting pycryptodome\n",
            "  Downloading pycryptodome-3.15.0-cp35-abi3-manylinux2010_x86_64.whl (2.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.3 MB 41.3 MB/s \n",
            "\u001b[?25hCollecting markdown-it-py[linkify,plugins]\n",
            "  Downloading markdown_it_py-2.1.0-py3-none-any.whl (84 kB)\n",
            "\u001b[K     |████████████████████████████████| 84 kB 2.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from gradio) (7.1.2)\n",
            "Collecting paramiko\n",
            "  Downloading paramiko-2.12.0-py2.py3-none-any.whl (213 kB)\n",
            "\u001b[K     |████████████████████████████████| 213 kB 61.5 MB/s \n",
            "\u001b[?25hCollecting httpx\n",
            "  Downloading httpx-0.23.0-py3-none-any.whl (84 kB)\n",
            "\u001b[K     |████████████████████████████████| 84 kB 2.8 MB/s \n",
            "\u001b[?25hCollecting uvicorn\n",
            "  Downloading uvicorn-0.19.0-py3-none-any.whl (56 kB)\n",
            "\u001b[K     |████████████████████████████████| 56 kB 4.2 MB/s \n",
            "\u001b[?25hCollecting orjson\n",
            "  Downloading orjson-3.8.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (272 kB)\n",
            "\u001b[K     |████████████████████████████████| 272 kB 52.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from gradio) (3.2.2)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from gradio) (6.0)\n",
            "Collecting websockets>=10.0\n",
            "  Downloading websockets-10.4-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (106 kB)\n",
            "\u001b[K     |████████████████████████████████| 106 kB 54.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: fsspec in /usr/local/lib/python3.7/dist-packages (from gradio) (2022.10.0)\n",
            "Collecting ffmpy\n",
            "  Downloading ffmpy-0.3.0.tar.gz (4.8 kB)\n",
            "Collecting h11<0.13,>=0.11\n",
            "  Downloading h11-0.12.0-py3-none-any.whl (54 kB)\n",
            "\u001b[K     |████████████████████████████████| 54 kB 1.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from gradio) (1.3.5)\n",
            "Collecting fastapi\n",
            "  Downloading fastapi-0.87.0-py3-none-any.whl (55 kB)\n",
            "\u001b[K     |████████████████████████████████| 55 kB 2.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from gradio) (1.21.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from gradio) (2.11.3)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.7/dist-packages (from gradio) (3.8.3)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.7/dist-packages (from gradio) (1.10.2)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.7/dist-packages (from aiohttp->gradio) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from aiohttp->gradio) (4.1.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp->gradio) (6.0.2)\n",
            "Requirement already satisfied: asynctest==0.13.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->gradio) (0.13.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->gradio) (1.8.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->gradio) (22.1.0)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->gradio) (2.1.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.7/dist-packages (from aiohttp->gradio) (4.0.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from aiohttp->gradio) (1.3.3)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.7/dist-packages (from yarl<2.0,>=1.0->aiohttp->gradio) (2.10)\n",
            "Collecting starlette==0.21.0\n",
            "  Downloading starlette-0.21.0-py3-none-any.whl (64 kB)\n",
            "\u001b[K     |████████████████████████████████| 64 kB 1.5 MB/s \n",
            "\u001b[?25hCollecting anyio<5,>=3.4.0\n",
            "  Downloading anyio-3.6.2-py3-none-any.whl (80 kB)\n",
            "\u001b[K     |████████████████████████████████| 80 kB 3.8 MB/s \n",
            "\u001b[?25hCollecting sniffio>=1.1\n",
            "  Downloading sniffio-1.3.0-py3-none-any.whl (10 kB)\n",
            "Collecting rfc3986[idna2008]<2,>=1.3\n",
            "  Downloading rfc3986-1.5.0-py2.py3-none-any.whl (31 kB)\n",
            "Collecting httpcore<0.16.0,>=0.15.0\n",
            "  Downloading httpcore-0.15.0-py3-none-any.whl (68 kB)\n",
            "\u001b[K     |████████████████████████████████| 68 kB 5.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from httpx->gradio) (2022.9.24)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->gradio) (2.0.1)\n",
            "Collecting mdurl~=0.1\n",
            "  Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
            "Collecting mdit-py-plugins\n",
            "  Downloading mdit_py_plugins-0.3.1-py3-none-any.whl (46 kB)\n",
            "\u001b[K     |████████████████████████████████| 46 kB 1.8 MB/s \n",
            "\u001b[?25hCollecting linkify-it-py~=1.0\n",
            "  Downloading linkify_it_py-1.0.3-py3-none-any.whl (19 kB)\n",
            "Collecting uc-micro-py\n",
            "  Downloading uc_micro_py-1.0.1-py3-none-any.whl (6.2 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->gradio) (2.8.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->gradio) (1.4.4)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->gradio) (0.11.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->gradio) (3.0.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib->gradio) (1.15.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->gradio) (2022.6)\n",
            "Collecting cryptography>=2.5\n",
            "  Downloading cryptography-38.0.3-cp36-abi3-manylinux_2_24_x86_64.whl (4.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.1 MB 36.3 MB/s \n",
            "\u001b[?25hCollecting pynacl>=1.0.1\n",
            "  Downloading PyNaCl-1.5.0-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (856 kB)\n",
            "\u001b[K     |████████████████████████████████| 856 kB 55.3 MB/s \n",
            "\u001b[?25hCollecting bcrypt>=3.1.3\n",
            "  Downloading bcrypt-4.0.1-cp36-abi3-manylinux_2_24_x86_64.whl (593 kB)\n",
            "\u001b[K     |████████████████████████████████| 593 kB 31.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.7/dist-packages (from cryptography>=2.5->paramiko->gradio) (1.15.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.12->cryptography>=2.5->paramiko->gradio) (2.21)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->gradio) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->gradio) (3.0.4)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.7/dist-packages (from uvicorn->gradio) (7.1.2)\n",
            "Building wheels for collected packages: ffmpy, python-multipart\n",
            "  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ffmpy: filename=ffmpy-0.3.0-py3-none-any.whl size=4709 sha256=efdb70764e79311d631dccc58dfa9370056a097f41de2c571e58ab90aa004d4d\n",
            "  Stored in directory: /root/.cache/pip/wheels/13/e4/6c/e8059816e86796a597c6e6b0d4c880630f51a1fcfa0befd5e6\n",
            "  Building wheel for python-multipart (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for python-multipart: filename=python_multipart-0.0.5-py3-none-any.whl size=31677 sha256=6eb35fa9837a665ea41b2716b0ddcb2c2cfbfbd117b1844cf0a8dda8749e11ed\n",
            "  Stored in directory: /root/.cache/pip/wheels/2c/41/7c/bfd1c180534ffdcc0972f78c5758f89881602175d48a8bcd2c\n",
            "Successfully built ffmpy python-multipart\n",
            "Installing collected packages: sniffio, mdurl, uc-micro-py, rfc3986, markdown-it-py, h11, anyio, starlette, pynacl, mdit-py-plugins, linkify-it-py, httpcore, cryptography, bcrypt, websockets, uvicorn, python-multipart, pydub, pycryptodome, paramiko, orjson, httpx, ffmpy, fastapi, gradio\n",
            "Successfully installed anyio-3.6.2 bcrypt-4.0.1 cryptography-38.0.3 fastapi-0.87.0 ffmpy-0.3.0 gradio-3.9.1 h11-0.12.0 httpcore-0.15.0 httpx-0.23.0 linkify-it-py-1.0.3 markdown-it-py-2.1.0 mdit-py-plugins-0.3.1 mdurl-0.1.2 orjson-3.8.1 paramiko-2.12.0 pycryptodome-3.15.0 pydub-0.25.1 pynacl-1.5.0 python-multipart-0.0.5 rfc3986-1.5.0 sniffio-1.3.0 starlette-0.21.0 uc-micro-py-1.0.1 uvicorn-0.19.0 websockets-10.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "0dSnTo4lAlqh",
        "outputId": "a7399b16-7a68-4f72-92b8-976276fa8cde",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WChSHcciElZc"
      },
      "source": [
        "with open('/content/drive/MyDrive/Models/Anxiety_Dict.p', 'rb') as handle:\n",
        "    anxiety_word_index = pickle.load(handle)\n",
        "\n",
        "with open('/content/drive/MyDrive/Models/Control_Dict.p', 'rb') as handle:\n",
        "    control_word_index = pickle.load(handle)\n",
        "\n",
        "with open('/content/drive/MyDrive/Models/Depression_Dict.p', 'rb') as handle:\n",
        "    depression_word_index = pickle.load(handle)\n",
        "\n",
        "with open('/content/drive/MyDrive/Models/Autism_Dict.p', 'rb') as handle:\n",
        "    autism_word_index = pickle.load(handle)\n",
        "\n",
        "with open('/content/drive/MyDrive/Models/BPD_Dict.p', 'rb') as handle:\n",
        "    BPD_word_index = pickle.load(handle)\n",
        "\n",
        "with open('/content/drive/MyDrive/Models/Bipolar_Dict.p', 'rb') as handle:\n",
        "    bipolar_word_index = pickle.load(handle)\n",
        "\n",
        "with open('/content/drive/MyDrive/Models/Schizo_Dict.p', 'rb') as handle:\n",
        "    schizo_word_index = pickle.load(handle)\n",
        "\n",
        "\n",
        "anxiety_model = tf.keras.models.load_model('/content/drive/MyDrive/Models/MHSMC_Model_Anxiety')\n",
        "control_model = tf.keras.models.load_model('/content/drive/MyDrive/Models/MHSMC_Model_Control')\n",
        "depression_model = tf.keras.models.load_model('/content/drive/MyDrive/Models/MHSMC_Model_Depression')\n",
        "autism_model = tf.keras.models.load_model('/content/drive/MyDrive/Models/MHSMC_Model_Autism')\n",
        "BPD_model = tf.keras.models.load_model('/content/drive/MyDrive/Models/MHSMC_Model_BPD')\n",
        "bipolar_model = tf.keras.models.load_model('/content/drive/MyDrive/Models/MHSMC_Model_Bipolar')\n",
        "schizo_model = tf.keras.models.load_model('/content/drive/MyDrive/Models/MHSMC_Model_Schizo')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sq2Tpw5YD_VB"
      },
      "source": [
        "def detokenize(tokens):\n",
        "    text = ' '.join(tokens)\n",
        "    return text\n",
        "\n",
        "def preprocess_corpus(tokens):\n",
        "  tokens = [t.lower() for t in tokens] # make lowercase\n",
        "\n",
        "  stop = set(stopwords.words('english'))\n",
        "  tokens = [t for t in tokens if t not in stop] # remove stop words\n",
        "  tokens = list(filter(None, tokens)) # get rid of extra spaces\n",
        "\n",
        "  text = detokenize(tokens)\n",
        "\n",
        "\n",
        "  #text = re.sub(r\"(\\w)([.,;:!?'\\\"”\\)])\", r\"\\1 \\2\", text) #put spaces between word and special characters\n",
        "  #text = re.sub(r\"([.,;:!?'\\\"“\\(])(\\w)\", r\"\\1 \\2\", text) #put spaces between special characters and word\n",
        "  #text = re.sub(r\"([.,;:!?'\\\"“\\(])(\\w)\", r\"([.,;:!?'\\\"“\\(])(\\w)\", text) #put spaces between special characters and other special characters\n",
        "\n",
        "  pattern = r\"[{}]\".format(string.punctuation) \n",
        "  text = re.sub(pattern, \"\", text)  # remove puntucation \n",
        "  text = re.sub(r'\\b\\d+\\b', '', text) # only accept text\n",
        "  \n",
        "  tokens = re.split(r\"\\s+\",text) #split the words into array\n",
        "  tokens = list(filter(None, tokens)) # get rid of extra spaces\n",
        "  \n",
        "  return tokens\n",
        "\n",
        "def getProcessCorpus(data):\n",
        "  processed_data = []\n",
        "  for i in range(len(data)):\n",
        "    sentence = data[i]\n",
        "    p_sentence = preprocess_corpus(sentence)\n",
        "    processed_data.append(p_sentence[1:len(p_sentence)])\n",
        "  return processed_data"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G_1SrDJ4E7qL"
      },
      "source": [
        "def runMentalHealthAlarmSystem(input_sentence):\n",
        "  processed_sentence = preprocess_corpus(input_sentence.split(' '))\n",
        "\n",
        "  x_anxiety = []\n",
        "  x_control = []\n",
        "  x_depression = []\n",
        "  x_autism = []\n",
        "  x_BPD = []\n",
        "  x_bipolar = []\n",
        "  x_schizo = []\n",
        "\n",
        "  x_anxiety_numeric = []\n",
        "  x_control_numeric = []\n",
        "  x_depression_numeric = []\n",
        "  x_autism_numeric = []\n",
        "  x_BPD_numeric = []\n",
        "  x_bipolar_numeric = []\n",
        "  x_schizo_numeric = []\n",
        "\n",
        "  for token in processed_sentence:\n",
        "    x_anxiety_numeric.append(anxiety_word_index[token])\n",
        "    x_control_numeric.append(control_word_index[token])\n",
        "    x_depression_numeric.append(depression_word_index[token])\n",
        "    x_autism_numeric.append(autism_word_index[token])\n",
        "    x_BPD_numeric.append(BPD_word_index[token])\n",
        "    x_bipolar_numeric.append(bipolar_word_index[token])\n",
        "    x_schizo_numeric.append(schizo_word_index[token])\n",
        "\n",
        "  x_anxiety.append(x_anxiety_numeric)\n",
        "  x_control.append(x_control_numeric)\n",
        "  x_depression.append(x_depression_numeric)\n",
        "  x_autism.append(x_autism_numeric)\n",
        "  x_BPD.append(x_BPD_numeric)\n",
        "  x_bipolar.append(x_bipolar_numeric)\n",
        "  x_schizo.append(x_schizo_numeric)\n",
        "\n",
        "  max_sequence_length = 5000\n",
        "\n",
        "  x_anxiety = np.array(pad_sequences(x_anxiety, maxlen = 4792, padding = 'pre'))\n",
        "  x_control = np.array(pad_sequences(x_control, maxlen = 4792, padding = 'pre'))\n",
        "  x_depression = np.array(pad_sequences(x_depression, maxlen = 8403, padding = 'pre'))\n",
        "  x_autism = np.array(pad_sequences(x_autism, maxlen = 3902, padding = 'pre'))\n",
        "  x_BPD = np.array(pad_sequences(x_BPD, maxlen = 2996, padding = 'pre'))\n",
        "  x_bipolar = np.array(pad_sequences(x_bipolar, maxlen = 4792, padding = 'pre'))\n",
        "  x_schizo = np.array(pad_sequences(x_schizo, maxlen = 2781, padding = 'pre'))\n",
        "\n",
        "  mental_health_stats = []\n",
        "\n",
        "  anxiety_percentage = (anxiety_model.predict(x_anxiety))\n",
        "  control_percentage = (control_model.predict(x_control))\n",
        "  depression_percentage = (depression_model.predict(x_depression))\n",
        "  autism_percentage = (autism_model.predict(x_autism))\n",
        "  BPD_percentage = (BPD_model.predict(x_BPD))\n",
        "  bipolar_percentage = (bipolar_model.predict(x_bipolar))\n",
        "  schizo_percentage = (schizo_model.predict(x_schizo))\n",
        "\n",
        "  mental_health_stats.append(\"{:.2%}\".format(anxiety_percentage[0][1]))\n",
        "  mental_health_stats.append(\"{:.2%}\".format(control_percentage[0][1]))\n",
        "  mental_health_stats.append(\"{:.2%}\".format(depression_percentage[0][1]))\n",
        "  mental_health_stats.append(\"{:.2%}\".format(autism_percentage[0][1]))\n",
        "  mental_health_stats.append(\"{:.2%}\".format(BPD_percentage[0][1]))\n",
        "  mental_health_stats.append(\"{:.2%}\".format(bipolar_percentage[0][1]))\n",
        "  mental_health_stats.append(\"{:.2%}\".format(schizo_percentage[0][1]))\n",
        "\n",
        "  df = pd.DataFrame(mental_health_stats, columns = ['Percentage Chance of Disorder'])\n",
        "  df['Disorder'] = ['Anxiety', 'Control', 'Depression', 'Autism', 'Borderline Personality', 'Bipolar', 'Schizo']\n",
        "  df['Description'] = ['Assess similarity to Reddit discussion of Anxiety', \n",
        "                       'Assess similarity to Reddit forum to discuss, vent, support and share information about mental health, illness, and wellness.', \n",
        "                       'Assess similarity to Reddit discussion of Depression',\n",
        "                       'Assess similarity to Reddit discussion of Autism',\n",
        "                       'Assess similarity to Reddit discussion of Borderline Personality Disorder',\n",
        "                       'Assess similarity to Reddit discussion of Bipolar',\n",
        "                       'Assess similarity to Reddit disccusion of Schizo']\n",
        "  \n",
        "  return df\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JxA2PR2uHZ1n",
        "outputId": "0888f10e-365f-4ea3-fe0b-3206c0cd89ef"
      },
      "source": [
        "sentence = 'I am depressed and upset, and I dont know what to do'\n",
        "df = runMentalHealthAlarmSystem(sentence)\n",
        "print(df)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 1s 600ms/step\n",
            "1/1 [==============================] - 0s 151ms/step\n",
            "1/1 [==============================] - 0s 102ms/step\n",
            "1/1 [==============================] - 0s 83ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7facfb20d7a0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 93ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7facfb1c3b90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 88ms/step\n",
            "1/1 [==============================] - 0s 78ms/step\n",
            "  Percentage Chance of Disorder                Disorder  \\\n",
            "0                         0.72%                 Anxiety   \n",
            "1                        34.18%                 Control   \n",
            "2                        96.03%              Depression   \n",
            "3                        38.39%                  Autism   \n",
            "4                        28.15%  Borderline Personality   \n",
            "5                        39.26%                 Bipolar   \n",
            "6                         2.83%                  Schizo   \n",
            "\n",
            "                                         Description  \n",
            "0  Assess similarity to Reddit discussion of Anxiety  \n",
            "1  Assess similarity to Reddit forum to discuss, ...  \n",
            "2  Assess similarity to Reddit discussion of Depr...  \n",
            "3   Assess similarity to Reddit discussion of Autism  \n",
            "4  Assess similarity to Reddit discussion of Bord...  \n",
            "5  Assess similarity to Reddit discussion of Bipolar  \n",
            "6   Assess similarity to Reddit disccusion of Schizo  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 643
        },
        "id": "WmOBjoBiPDm4",
        "outputId": "e9d20d2c-4388-46f6-cc75-ec8f2b36e427"
      },
      "source": [
        "iface = gr.Interface(fn=runMentalHealthAlarmSystem, inputs=\"text\", outputs=\"dataframe\")\n",
        "iface.launch(share=True)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set `debug=True` in `launch()`\n",
            "Running on public URL: https://67e8b143a7982bf5.gradio.app\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades (NEW!), check out Spaces: https://huggingface.co/spaces\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://67e8b143a7982bf5.gradio.app\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<gradio.routes.App at 0x7facf7840cd0>,\n",
              " 'http://127.0.0.1:7861/',\n",
              " 'https://67e8b143a7982bf5.gradio.app')"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    }
  ]
}